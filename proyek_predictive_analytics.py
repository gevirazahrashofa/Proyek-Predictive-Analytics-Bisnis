# -*- coding: utf-8 -*-
"""Proyek Predictive Analytics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gpjnLc2cDofCG9evaJLbGlzNqGfysJ88

# **Proyek Predictive Analytics**

Nama : Gevira Zahra Shofa

**Data Collection**
"""

# Import library
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
df = pd.read_csv("House_Rent_Dataset.csv")

"""**Data Understanding & Removing Outlier**"""

# Informasi data
print("Jumlah baris dan kolom:", df.shape)
print("\nInfo dataset:")
print(df.info())
print("\nStatistik deskriptif:")
print(df.describe())
print("\nJumlah missing values per kolom:")
print(df.isnull().sum())

# Untuk fitur Posted On dan Point of Contract tidak mempengaruhi harga sewa model jadi akan saya hapus
df = df.drop(['Posted On', 'Point of Contact'], axis = 'columns')

"""**Univariate Analysis**"""

# Cek distribusi jumlah data pada fitur 'Area Type'
print("Distribusi Area Type sebelum pembersihan:")
print(df['Area Type'].value_counts())

# Hapus baris yang memiliki 'Area Type' = 'Built Area' karena hanya ada 2 sampel
df = df[df['Area Type'] != 'Built Area']

print("\nDistribusi Area Type setelah pembersihan:")
print(df['Area Type'].value_counts())

# Tampilkan ringkasan jumlah data untuk beberapa fitur kategorikal
for col in ['City', 'Furnishing Status', 'Tenant Preferred']:
    print(f"\nDistribusi kategori pada kolom {col}:")
    print(df[col].value_counts())

# Tampilkan jumlah nilai unik untuk 'Floor' dan 'Area Locality'
print("\nJumlah kategori unik pada 'Floor':", df['Floor'].nunique())
print("Jumlah kategori unik pada 'Area Locality':", df['Area Locality'].nunique())

# Drop kolom yang terlalu kompleks atau tidak berguna untuk prediksi
df.drop(columns=['Floor', 'Area Locality'], inplace=True)

# Tampilkan 5 baris awal dataset setelah pembersihan
print(df.head())

# Visualisasi distribusi semua fitur numerik dengan histogram
df.hist(bins=50, figsize=(12, 10), color='skyblue', edgecolor='black')
plt.suptitle("Distribusi Fitur Numerik", fontsize=16)
plt.tight_layout()
plt.show()

# Ringkasan statistik untuk kolom Rent
print("\nStatistik deskriptif untuk kolom 'Rent':")
print(df['Rent'].describe().apply(lambda val: f"{val:,.2f}"))

"""**Multivariate Analysis**"""

# Menambahkan fitur baru yaitu 'Price per sqft' dengan rumus Rent dikali 1000 dibagi Size
df['Price_per_sqft'] = (df['Rent'] * 1000) / df['Size']

# Mendeteksi outlier berdasarkan ukuran per BHK
# Misalnya, ukuran lebih kecil dari 300 sqft per BHK, nah berarti dianggap tidak biasa
df[(df.Size / df.BHK) < 300].head()

# Menampilkan dimensi data sebelum pemrosesan
df.shape

# Menghapus outlier berdasarkan ukuran per BHK
df_cleaned = df[~(df.Size / df.BHK < 300)]
df_cleaned.head()

# Menampilkan dimensi data setelah penghapusan outlier
df_cleaned.shape

#  Menganalisis statistik deskriptif dari 'Price per sqft'
df_cleaned.Price_per_sqft.describe().apply(lambda x: format(x, 'f'))

# Fungsi untuk menghapus outlier berdasarkan price per sqft
def remove_outliers_by_price(df):
    df_out = pd.DataFrame()
    for city, sub_df in df.groupby('City'):
        mean_price = np.mean(sub_df['Price_per_sqft'])
        std_dev = np.std(sub_df['Price_per_sqft'])
        filtered_df = sub_df[(sub_df['Price_per_sqft'] > (mean_price - std_dev)) & (sub_df['Price_per_sqft'] <= (mean_price + std_dev))]
        df_out = pd.concat([df_out, filtered_df], ignore_index=True)
    return df_out

# Menghapus outlier berdasarkan 'Price per sqft'
df_without_outliers = remove_outliers_by_price(df_cleaned)
df_without_outliers.shape

# Mendeteksi outlier pada jumlah kamar mandi
# Misalnya, jika jumlah kamar mandi lebih dari BHK + 2, dianggap outlier
df_without_outliers[df_without_outliers.Bathroom > (df_without_outliers.BHK + 2)]

# Menghapus outlier berdasarkan jumlah kamar mandi
df_without_outliers = df_without_outliers[~(df_without_outliers.Bathroom > (df_without_outliers.BHK + 2))]
df_without_outliers.head()

# Menampilkan dimensi data setelah pembersihan
df_without_outliers.shape

# Menghapus kolom 'Price_per_sqft' karena sudah tidak diperlukan lagi
df_final = df_without_outliers.drop(columns=['Price_per_sqft'])

# Menganalisis korelasi antar fitur numerik dan target 'Rent'
plt.figure(figsize=(10, 8))
# Drop non-numeric columns before calculating correlation
corr_matrix = df_final.select_dtypes(include=['number']).corr().round(2)

# Membuat heatmap untuk korelasi antar fitur numerik
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title("Matriks Korelasi antara Fitur Numerik dan 'Rent'", fontsize=20)

# Menganalisis hubungan antara fitur kategorikal dan target 'Rent'
categorical_features = df_without_outliers.select_dtypes(include='object').columns.tolist()

# Membuat grafik bar untuk fitur kategorikal terhadap 'Rent' dengan penyesuaian palet warna dan hue
for feature in categorical_features:
    sns.catplot(x=feature, y="Rent", kind="bar", hue=feature, dodge=False, height=4, aspect=3, data=df_without_outliers, palette="Set2", legend=False)
    plt.title(f"Rata-rata 'Rent' Relatif terhadap {feature}")

"""**Data Preparation**"""

# One-Hot Encoding untuk fitur kategorikal
# Menggunakan pd.get_dummies untuk mengubah fitur kategorikal menjadi kolom biner
df_encoded = pd.get_dummies(df_final, drop_first=True)

# Menampilkan 5 baris pertama untuk memastikan perubahan
print("\nContoh data setelah One-Hot Encoding:")
print(df_encoded.head())

# Pembagian Data untuk Training dan Testing
from sklearn.model_selection import train_test_split

# Menentukan fitur (X) dan target (y)
X = df_encoded.drop(columns=['Rent'])  # Fitur yang digunakan untuk memprediksi
y = df_encoded['Rent']  # Target variabel (Harga Sewa)

# Membagi data menjadi data training dan testing (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("\nDimensi data setelah pembagian:")
print(f"Training data (X_train, y_train): {X_train.shape}, {y_train.shape}")
print(f"Testing data (X_test, y_test): {X_test.shape}, {y_test.shape}")

# Normalisasi Fitur Numerik
from sklearn.preprocessing import MinMaxScaler

# Normalisasi data numerik menggunakan MinMaxScaler agar setiap fitur berada dalam rentang yang sama (0-1)
scaler = MinMaxScaler()

# Normalisasi hanya kolom numerik yang tidak terpengaruh oleh One-Hot Encoding
numerical_features = ['Size', 'BHK', 'Bathroom']
X_train[numerical_features] = scaler.fit_transform(X_train[numerical_features])
X_test[numerical_features] = scaler.transform(X_test[numerical_features])

# Menampilkan data setelah normalisasi
print("\nContoh data setelah normalisasi (X_train):")
print(X_train.head())

"""**Modelling**"""

# Importing libraries
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import GridSearchCV

# Membuat Model dengan Linear Regression
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)

# Prediksi menggunakan model Linear Regression
lr_predictions = lr_model.predict(X_test)

# Evaluasi model Linear Regression
lr_mse = mean_squared_error(y_test, lr_predictions)
# Calculate RMSE by taking the square root of MSE
lr_rmse = np.sqrt(lr_mse)
lr_mae = mean_absolute_error(y_test, lr_predictions)
lr_r2 = r2_score(y_test, lr_predictions)

print("Evaluasi Model Linear Regression:")
print(f"Mean Squared Error (MSE): {lr_mse:.2f}")
print(f"Root Mean Squared Error (RMSE): {lr_rmse:.2f}")
print(f"Mean Absolute Error (MAE): {lr_mae:.2f}")
print(f"R^2 Score: {lr_r2:.2f}")

# Membuat Model dengan Random Forest Regressor
rf_model = RandomForestRegressor(random_state=42)
rf_model.fit(X_train, y_train)

# Prediksi menggunakan model Random Forest
rf_predictions = rf_model.predict(X_test)

# Evaluasi model Random Forest
rf_mse = mean_squared_error(y_test, rf_predictions)
# Calculate RMSE by taking the square root of MSE manually
rf_rmse = np.sqrt(rf_mse)
rf_mae = mean_absolute_error(y_test, rf_predictions)
rf_r2 = r2_score(y_test, rf_predictions)

print("\nEvaluasi Model Random Forest Regressor:")
print(f"Mean Squared Error (MSE): {rf_mse:.2f}")
print(f"Root Mean Squared Error (RMSE): {rf_rmse:.2f}")
print(f"Mean Absolute Error (MAE): {rf_mae:.2f}")
print(f"R^2 Score: {rf_r2:.2f}")

from sklearn.ensemble import GradientBoostingRegressor

# Membuat Model dengan Gradient Boosting Regressor
gb_model = GradientBoostingRegressor(random_state=42)
gb_model.fit(X_train, y_train)

# Prediksi menggunakan model Gradient Boosting
gb_predictions = gb_model.predict(X_test)

# Evaluasi model Gradient Boosting
gb_mse = mean_squared_error(y_test, gb_predictions)
# Calculate RMSE by taking the square root of MSE
gb_rmse = np.sqrt(gb_mse)  # Calculate RMSE manually
gb_mae = mean_absolute_error(y_test, gb_predictions)
gb_r2 = r2_score(y_test, gb_predictions)

print("\nEvaluasi Model Gradient Boosting Regressor:")
print(f"Mean Squared Error (MSE): {gb_mse:.2f}")
print(f"Root Mean Squared Error (RMSE): {gb_rmse:.2f}")
print(f"Mean Absolute Error (MAE): {gb_mae:.2f}")
print(f"R^2 Score: {gb_r2:.2f}")

# Hyperparameter Tuning untuk Random Forest Regressor menggunakan GridSearchCV
param_grid_rf = {
    'n_estimators': [50, 100, 200],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_search_rf = GridSearchCV(estimator=rf_model, param_grid=param_grid_rf, cv=5, scoring='neg_mean_squared_error')
grid_search_rf.fit(X_train, y_train)

# Menampilkan hasil hyperparameter tuning Random Forest
print("\nHasil GridSearchCV untuk Random Forest Regressor:")
print(f"Best Parameters: {grid_search_rf.best_params_}")
print(f"Best Score (MSE): {-grid_search_rf.best_score_:.2f}")

# Menggunakan model terbaik dari GridSearchCV untuk Random Forest
best_rf_model = grid_search_rf.best_estimator_

# Prediksi dengan model terbaik
best_rf_predictions = best_rf_model.predict(X_test)

# Evaluasi model terbaik Random Forest
best_rf_mse = mean_squared_error(y_test, best_rf_predictions)
# Calculate RMSE by taking the square root of MSE manually, instead of using 'squared=False'
best_rf_rmse = np.sqrt(best_rf_mse)
best_rf_mae = mean_absolute_error(y_test, best_rf_predictions)
best_rf_r2 = r2_score(y_test, best_rf_predictions)

print("\nEvaluasi Model Random Forest Regressor setelah Hyperparameter Tuning:")
print(f"Mean Squared Error (MSE): {best_rf_mse:.2f}")
print(f"Root Mean Squared Error (RMSE): {best_rf_rmse:.2f}")
print(f"Mean Absolute Error (MAE): {best_rf_mae:.2f}")
print(f"R^2 Score: {best_rf_r2:.2f}")

# Memilih Model Terbaik
# Menyusun hasil evaluasi dan memilih model dengan R^2 terbaik
models = {
    "Linear Regression": lr_r2,
    "Random Forest Regressor": best_rf_r2 if best_rf_r2 > rf_r2 else rf_r2,
    "Gradient Boosting Regressor": gb_r2
}

best_model_name = max(models, key=models.get)

print(f"\nModel terbaik adalah: {best_model_name}")

"""**Evaluation**"""

# Importing evaluation metrics
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np

# Evaluasi untuk 3 model
models = {
    "Linear Regression": lr_model,
    "Random Forest Regressor": rf_model,
    "Gradient Boosting Regressor": gb_model
}

# Function for evaluating models
def evaluate_model(model, X_test, y_test):
    predictions = model.predict(X_test)
    mse = mean_squared_error(y_test, predictions)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_test, predictions)
    r2 = r2_score(y_test, predictions)
    return mse, rmse, mae, r2

# Evaluasi semua model
results = {}

for model_name, model in models.items():
    mse, rmse, mae, r2 = evaluate_model(model, X_test, y_test)
    results[model_name] = {
        "MSE": mse,
        "RMSE": rmse,
        "MAE": mae,
        "R^2": r2
    }

# Tampilkan hasil evaluasi untuk semua model
for model_name, result in results.items():
    print(f"\nEvaluasi Model: {model_name}")
    print(f"Mean Squared Error (MSE): {result['MSE']:.2f}")
    print(f"Root Mean Squared Error (RMSE): {result['RMSE']:.2f}")
    print(f"Mean Absolute Error (MAE): {result['MAE']:.2f}")
    print(f"R^2 Score: {result['R^2']:.2f}")